# -*- coding: utf-8 -*-
"""fine tuning project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1doDPi7ShdQFTsxeqXiJ_OkKi1GPeIFDG

# 1. Define Your Domain and Task

The task objective: Sentiment analysis of Amazon reviews from 1 to 5 star

The expected output: 1 to 5 star labels

Dataset: "yelp_review_full"

# 2. Dataset Preparation
"""

import re
import os
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
!pip install datasets
from datasets import load_dataset, DatasetDict
from transformers import Trainer, TrainingArguments
from sklearn.metrics import classification_report
import pandas as pd
from collections import Counter

big_dataset = load_dataset("yelp_review_full")
# take approximately 1/20th of the dataset since the full dataset takes too long to train on
small_train = big_dataset['train'].train_test_split(test_size=1/20, seed=1)
small_test = big_dataset['test'].train_test_split(test_size=1/20, seed=1)
dataset = DatasetDict({"train": small_train['test'], "test": small_test['test']})

"""Data Collection: Gather a dataset with at least 200 entries. Ensure the data is specific to your domain."""

print(f"Train size: {len(dataset['train'])}")
print(f"Test size: {len(dataset['test'])}")

"""Cleaning:
- Remove duplicates, irrelevant entries, or junk text.
- Standardize text formatting (e.g., lowercase all text, normalize dates).
- Fix typos and grammatical inconsistencies.
"""

train_df = pd.DataFrame(dataset['train'])
test_df = pd.DataFrame(dataset['test'])
print(f"Duplicates in train: {train_df.duplicated(subset='text').sum()}")
print(f"Duplicates in test: {test_df.duplicated(subset='text').sum()}")

def clean_text(example):
    example["text"] = example["text"].lower()
    # keep only alphanumeric characters, spaces, and punctuation
    example["text"] = re.sub(r"[^\w\s.,!?]", "", example["text"])
    return example
dataset = dataset.map(clean_text)

"""Labeling:
- For classification tasks, ensure each entry is labeled accurately.
- For summarization tasks, ensure each entry includes a concise, high-quality summary.
"""

# too many to ensure all are labeled accurately

"""Balance the Dataset: Ensure there is no significant class imbalance. For instance, if you’re working on sentiment analysis, have a roughly equal number of positive, neutral, and negative samples."""

train_labels = [example['label'] for example in dataset['train']]
test_labels = [example['label'] for example in dataset['test']]
distribution = {
    "Class": ["1-star", "2-star", "3-star", "4-star", "5-star"],
    "Train Count": [train_labels.count(i) for i in range(5)],
    "Test Count": [test_labels.count(i) for i in range(5)]
}
df = pd.DataFrame(distribution)
print(df)

"""# 3. Fine-Tune the Model"""

from google.colab import drive
drive.mount('/content/drive')

"""Step 1: Environment Setup"""

# Check for GPU availability
print(torch.cuda.is_available()) # Should return True

"""Step 2: Load the Pre-trained Model"""

model_name = "distilbert-base-uncased"
model_checkpoint_dir = "/content/drive/MyDrive/model_checkpoint"

# if training from start:
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# if opening saved model:
#model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_dir)
#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_dir)

"""Step 3: Preprocess the Dataset"""

def preprocess_function(examples):
  return tokenizer(examples['text'], truncation=True, padding=True)

tokenized_dataset = dataset.map(preprocess_function, batched=True)

"""Step 4: Define Training Arguments"""

training_args = TrainingArguments(
  output_dir="./results",
  save_steps=500,
  save_total_limit=3,
  evaluation_strategy="epoch",
  learning_rate=3e-5,
  per_device_train_batch_size=16,
  num_train_epochs=3,
  weight_decay=0.01,
  gradient_accumulation_steps=2,
  warmup_steps=500,
)
trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=tokenized_dataset["train"],
  eval_dataset=tokenized_dataset["test"],
)

"""Step 5: Train the Model"""

# if training from start:
trainer.train()
# if opening saved model:
#trainer.train(resume_from_checkpoint=True)

"""Step 6: Save the Fine-Tuned Model"""

model.save_pretrained(model_checkpoint_dir)
tokenizer.save_pretrained(model_checkpoint_dir)
!cp -r ./results /content/drive/MyDrive/results

"""# 4. Model Evaluation"""

results = trainer.evaluate()
print(results)

"""Detailed metrics with sklearn"""

predictions = trainer.predict(tokenized_dataset["test"])
y_pred = predictions.predictions.argmax(axis=1)
y_true = tokenized_dataset["test"]["label"]
print(classification_report(y_true, y_pred, digits=4))

"""# Analysis and Report

- Dataset Insights: Describe your dataset, including how it was cleaned and labeled.
  
  A: The dataset is "yelp_review_full".

  Input: Yelp reviews (text)

  Output: Star review (0 meaning 1-star, 4 meaning 5-star) (integer)

  Cleaning process:
  - Confirming there are no duplicates and balanced data
  - Remove special characters and lowercase all text
  - Sample 1/20th of the dataset to speed up training to a reasonable GPU usage

- Training Process: Summarize the steps you took to fine-tune the model.
  
  A:
  - Load pre-trained model "distilbert-base-uncased"
  - Load and tokenize yelp dataset
  - Define training arguments:
    
    learning_rate=3e-5 (higher learning rate for faster convergence)
    
    per_device_train_batch_size=16,
    
    num_train_epochs=3,
    
    weight_decay=0.01 (regularization)
    
    gradient_accumulation_steps=2 (simulate a larger batch size)
    
    warmup_steps=500 (stabilize convergence)

- Evaluation Results: Present your evaluation metrics and discuss the model’s strengths and weaknesses.

  ```python            
  precision    recall  f1-score   support

            0     0.7445    0.7255    0.7349       510
            1     0.5485    0.5553    0.5519       479
            2     0.5749    0.5591    0.5669       508
            3     0.5254    0.5918    0.5566       490
            4     0.7415    0.6823    0.7107       513

      accuracy                         0.6240      2500
    macro avg     0.6269    0.6228    0.6242      2500
  weighted avg     0.6289    0.6240    0.6258      2500
  ```

  The model performs the best (has the best recall and f1 scores) on 1 and 5 star reviews, but struggles to distunguish between moderate sentiments.

- Application and Impact: Explain how this fine-tuned model could be used in a real-world application. Include at least one potential improvement for future iterations.
  
  In the real world, this model has various applications in customer service and online shopping. Amazon is already using large language models to summarize customer sentiment about products based on a 5 star review system. This model could potentially benefit from separating the moderate reviews from the extreme ones and performing a 3 class multi-class classification on them alone.
"""

